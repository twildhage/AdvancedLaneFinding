#import mkl
#mkl.get_cpu_clocks()


import numpy as np
import matplotlib.pyplot as plt
from moviepy.editor import VideoFileClip
import cv2
from tqdm import tqdm

def random_affine_transformation(image, angle, shear_range=200):
    """
    The following code is adapted from:
    https://medium.com/@ksakmann/behavioral-cloning-make-a-car-drive-like-yourself-dc6021152713#.7k8vfppvk
    The function performs an affine transformation on the original image. This results in a
    new image, which still has a valid perspective but with a significantly altered
    steering angle. This allows to generate any real valued steering angle within
    the valid range defined by the shearing range. The main result is a smooth
    distributed dataset w.r.t. the steering angle.

    :image:
        source image on which the shear operation will be applied
    :angle:
        steering angle of the image
    :shear_range:
        random shear between [-shear_range, shear_range] will be applied
    :return:
        The image generated by applying random shear on the source image
    """
    rows, cols = image.shape[0:2]
    dx = np.random.randint(-shear_range, shear_range)
    random_point = [cols/2 + dx, rows/2]
    triangle1 = np.float32([[0,         rows],
                            [cols,      rows],
                            [cols/2,    rows/2]])
    triangle2 = np.float32([[0,    rows],
                            [cols, rows],
                            random_point])

    steering_correction = dx / (rows / 2) * 360 / (2 * np.pi * 25.0) / 6.0
    transf_matrix = cv2.getAffineTransform(triangle1, triangle2)
    #print(triangle2[2,0]-triangle1[2,0])
    #print(transf_matrix)
    image = cv2.warpAffine(image, transf_matrix, (cols, rows), borderMode=1)
    angle += steering_correction

    return image, angle


def lane_detection(img, kernel_size=3, thresholds_dict={'gradient':(30,100), 'magnitude':(70,100), 'direction':(0.8, 0.9),
                       's_channel':(100,255), 'r_channel':(150,255), 'u_channel':(140,180)}, verbose=1):

    def binary_threshold(scaled_image, thresholds=(20, 140)):
        binary_image = np.zeros_like(scaled_image)
        binary_image[(scaled_image > thresholds[0]) & (scaled_image <= thresholds[1])] = 1
        return binary_image
    
    def scale(image, max_value=255):
        normalized = np.absolute(image)/np.max(np.absolute(image))
        return (max_value * normalized).astype(type(max_value))
    
    # Convert to grayscale
    gray = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY)
    
    # HLS colour
    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)
    S = hls[:,:,2]
    binary_s = binary_threshold(scale(S), thresholds_dict['s_channel'])
    
    # RGB colour
    R = img[:,:,0]
    binary_r = binary_threshold(scale(R), thresholds_dict['r_channel'])

    channel = gray
    
    # Apply Sobel algorithm the get x,y gradients 
    gradx = cv2.Sobel(channel, cv2.CV_64F, 1, 0, ksize=kernel_size)
    grady = cv2.Sobel(channel, cv2.CV_64F, 0, 1, ksize=kernel_size)
    
    # Binary gradient in x-direction
    binary_gradx = binary_threshold(scale(gradx), thresholds_dict['gradient'])
    
    # Binary gradient in x-direction
    binary_grady = binary_threshold(scale(grady), thresholds_dict['gradient'])
    
    # Gradient x,y magnitude
    mag_gradxy = np.sqrt(gradx**2 + grady**2)
    binary_mag_gradxy = binary_threshold(scale(mag_gradxy), thresholds_dict['magnitude'])
    
    # Get binary direction
    dir_gradxy = np.arctan2(np.absolute(grady), np.absolute(gradx))
    binary_dir = binary_threshold(scale(dir_gradxy, 1.0), thresholds_dict['direction'])
    
    binary_combined = np.zeros_like(binary_dir)
    binary_combined[((binary_gradx == 1)      & (binary_grady == 1)) |
             ((binary_mag_gradxy == 1) & (binary_dir == 1)) |
                   ((binary_r == 1) &  (binary_s ==1))] = 1
    
    if verbose==1:
        fig, axes = plt.subplots(3,2, figsize=(20,10))
        axes[0,0].imshow(scale(mag_gradxy), cmap='hot')
        axes[0,0].set_title("magnitude")
        axes[0,1].imshow(scale(dir_gradxy, 1.0), cmap='hot')
        axes[0,1].set_title("direction")
        axes[1,0].imshow(scale(gradx), cmap='hot')
        axes[1,0].set_title("gradient x")
        axes[1,1].imshow(scale(grady), cmap='hot')
        axes[1,1].set_title("gradient y")
        axes[2,0].imshow(scale(R), cmap='hot')
        axes[2,0].set_title("R channel")
        axes[2,1].imshow(scale(S), cmap='hot')
        axes[2,1].set_title("S channel")
        plt.show()
    
        fig, axes = plt.subplots(3,2, figsize=(20,10))
        axes[0,0].imshow(binary_mag_gradxy, cmap='gray')
        axes[0,0].set_title("magnitude")
        axes[0,1].imshow(binary_dir, cmap='gray')
        axes[0,1].set_title("direction")
        axes[1,0].imshow(binary_gradx, cmap='gray')
        axes[1,0].set_title("gradient x")
        axes[1,1].imshow(binary_grady, cmap='gray')
        axes[1,1].set_title("gradient y")
        axes[2,0].imshow(binary_r, cmap='gray')
        axes[2,0].set_title("binary r")
        axes[2,1].imshow(binary_s, cmap='gray')
        axes[2,1].set_title("binary s")
        plt.show()
    
        
    return binary_combined

def get_calibration_parameters(images, verbose=1):
    
    # Use cv2.calibrateCamera() and cv2.undistort()
    imgpoints = [] # 2D
    objpoints = [] # 3D
    nx = 9
    ny = 6
    objp = np.zeros((nx*ny, 3), np.float32)
    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)

    distortion_coefficients = []
    camera_matrices = []
    singuar_values = []
    
    for number, img in enumerate(images):
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        # Finding chessboard corners (for an 9x6 board):
        ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)
        if verbose==1:
            print("Image: {0}\t found corners: {1}".format(number, ret))

        if ret==True:
            imgpoints.append(corners)
            objpoints.append(objp)

            # Drawing detected corners on an image:
            img = cv2.drawChessboardCorners(img, (nx,ny), corners, ret)

            # Camera calibration, given object points, image points, and the shape of the grayscale image:
            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)

            camera_matrices.append(mtx)
            distortion_coefficients.append(dist)
    
    for cm in camera_matrices:
        U, s, V = np.linalg.svd(cm, full_matrices=True)
        singuar_values.append(s)
        
    distortion_coefficients = np.array(distortion_coefficients).squeeze()
    camera_matrices = np.array(camera_matrices).squeeze()
    singuar_values = np.array(singuar_values)
    
    return camera_matrices, distortion_coefficients, singuar_values

def calibrate_images(images, calibrated_images, mtx, dist, verbose=1):
    for key in images:
        if verbose==1:
            print("Calibrate '{0}' images".format(key))
        for img in tqdm(images[key]):
            dst = cv2.undistort(img, mtx, dist, None, mtx)
            calibrated_images[key].append(dst)
    return calibrated_images


def lane_func(par, x):
    """
    This function returns the error according to a quadratic model 
    """
    return (par[0] * x**2 + par[1] * x + par[2]) 

def error_func(par, func, x, y):
    return func(par, x) - y

def robust_lane_fit(x, y, verbose=0):
    """
    This function performs a robust least square fit according to a 
    linear model. The term 'robust' refers to the insensitivity to outliners.
    The names 'll', 'rl' rever to left and right lines.
    """
    
    # initialize parameters par: (slope, bias)
    par0 = np.array([-0.5, 1, 1])
    # Set f_scale to 0.1 which means that inlier residuals are approximately lower than 0.1.
    f_scale = 0.1
    
    # Run robust least squares with loss='soft_l1', 
    res_robust = least_squares(error_func, par0, args=(lane_func, x, y), 
                               loss='soft_l1', f_scale=f_scale, verbose=verbose)
    # Get parameters 
    #a, b, c = res_robust.x
    # Calculate average line at given x coordinates
    #x_fit = np.array([0, x_range])
    #y_fit = a * x_fit**2 + b * x_fit + c
    # Reorganize x,y coordinates of points as line
    lane = []
    return res_robust

def load_video(filename):
    video = VideoFileClip(filename)
    images = [frame for frame in video.iter_frames(progress_bar=True)]
    
    return images




def find_lines(binary_warped):
    # Assuming you have created a warped binary image called "binary_warped"
    # Take a histogram of the bottom half of the image
    histogram = np.sum(binary_warped[binary_warped.shape[0]/2:,:], axis=0)
    # Create an output image to draw on and  visualize the result
    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255
    # Find the peak of the left and right halves of the histogram
    # These will be the starting point for the left and right lines
    midpoint = np.int(histogram.shape[0]/2)
    leftx_base = np.argmax(histogram[:midpoint])
    rightx_base = np.argmax(histogram[midpoint:]) + midpoint

    # Choose the number of sliding windows
    nwindows = 9
    # Set height of windows
    window_height = np.int(binary_warped.shape[0]/nwindows)
    # Identify the x and y positions of all nonzero pixels in the image
    nonzero = binary_warped.nonzero()
    nonzeroy = np.array(nonzero[0])
    nonzerox = np.array(nonzero[1])
    # Current positions to be updated for each window
    leftx_current = leftx_base
    rightx_current = rightx_base
    # Set the width of the windows +/- margin
    margin = 100
    # Set minimum number of pixels found to recenter window
    minpix = 50
    # Create empty lists to receive left and right lane pixel indices
    left_lane_inds = []
    right_lane_inds = []

    # Step through the windows one by one
    for window in range(nwindows):
        # Identify window boundaries in x and y (and right and left)
        win_y_low = binary_warped.shape[0] - (window+1)*window_height
        win_y_high = binary_warped.shape[0] - window*window_height
        win_xleft_low = leftx_current - margin
        win_xleft_high = leftx_current + margin
        win_xright_low = rightx_current - margin
        win_xright_high = rightx_current + margin
        # Draw the windows on the visualization image
        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) 
        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) 
        # Identify the nonzero pixels in x and y within the window
        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]
        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]
        # Append these indices to the lists
        left_lane_inds.append(good_left_inds)
        right_lane_inds.append(good_right_inds)
        # If you found > minpix pixels, recenter next window on their mean position
        if len(good_left_inds) > minpix:
            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))
        if len(good_right_inds) > minpix:        
            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))

    # Concatenate the arrays of indices
    left_lane_inds = np.concatenate(left_lane_inds)
    right_lane_inds = np.concatenate(right_lane_inds)

    # Extract left and right line pixel positions
    leftx = nonzerox[left_lane_inds]
    lefty = nonzeroy[left_lane_inds] 
    rightx = nonzerox[right_lane_inds]
    righty = nonzeroy[right_lane_inds] 

    # Fit a second order polynomial to each
    left_fit = np.polyfit(lefty, leftx, 2)
    right_fit = np.polyfit(righty, rightx, 2)
    
    
    
    return left_fit, right_fit
    
    